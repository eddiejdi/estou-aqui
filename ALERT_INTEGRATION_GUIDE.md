# üö® Alert Integration Guide - Estou Aqui + Prometheus + AlertManager

**Data:** 2026-02-16  
**Status:** ‚úÖ Production Ready  
**Version:** 1.0

---

## üìã Overview

Integra√ß√£o completa do pipeline de alertas Prometheus + AlertManager com o app **Estou Aqui**, permitindo que:

1. **AlertManager** dispara alertas baseado em m√©tricas (disco, CPU, mem√≥ria)
2. **Backend (Node.js)** recebe webhooks em tempo real
3. **Socket.io** transmite alertas para pain√©is/apps conectados
4. **Agent Bus** publica eventos para o sistema de agentes
5. **UI (Flutter/Web)** exibe alertas em tempo real

---

## üîß Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Prometheus (Metrics Collection) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ AlertManager (Rule Evaluation)   ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ Carregou 4 rules ‚úÖ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                 ‚îÇ
    ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Agent API     ‚îÇ  ‚îÇ  Estou Aqui Backend         ‚îÇ
‚îÇ :8503/alerts   ‚îÇ  ‚îÇ :3000/api/alerts/webhook    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                            ‚îÇ
    ‚îÇ                            ‚ñº
    ‚îÇ                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                ‚îÇ  Alert Processing     ‚îÇ
    ‚îÇ                ‚îÇ  - Parsing            ‚îÇ
    ‚îÇ                ‚îÇ  - Caching            ‚îÇ
    ‚îÇ                ‚îÇ  - History            ‚îÇ
    ‚îÇ                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                            ‚îÇ
    ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ          ‚îÇ                 ‚îÇ                 ‚îÇ
    ‚ñº          ‚ñº                 ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Bus  ‚îÇ  ‚îÇSocket.io ‚îÇ  ‚îÇ REST API    ‚îÇ  ‚îÇ Agent Bus Publish‚îÇ
‚îÇ      ‚îÇ  ‚îÇ/alerts   ‚îÇ  ‚îÇ /api/alerts ‚îÇ  ‚îÇ                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ              ‚îÇ
              ‚ñº              ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Frontend Clients      ‚îÇ
         ‚îÇ - Flutter App           ‚îÇ
         ‚îÇ - Web Dashboard         ‚îÇ
         ‚îÇ - Monitoring Screens    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Componentes Implementados

### 1. **Backend Services** (Node.js)

#### a) `services/alerting.js` - Core Alert Processing
- Recebe webhooks do AlertManager
- Processa e valida alertas
- Mant√©m cache de alertas ativos
- Armazena hist√≥rico (√∫ltimas 100)
- Publica no Agent Bus

**M√©todo Principal:**
```javascript
alertingService.processAlertManagerWebhook(payload)
```

#### b) `routes/alerts.js` - REST API
```
POST   /api/alerts/webhook         ‚Üê Recebe AlertManager
GET    /api/alerts/active          ‚Üê Alertas ativos agora
GET    /api/alerts/history?limit=50 ‚Üê Hist√≥rico
GET    /api/alerts/stats           ‚Üê Estat√≠sticas
DELETE /api/alerts/clear?hours=24  ‚Üê Limpeza
```

#### c) `services/alert-socket.js` - Socket.io Real-time
**Namespace:** `/alerts`

**Events (Server ‚Üí Client):**
```javascript
'alerts:update'    // Novo alerta disparado
'alert:critical'   // Alerta cr√≠tico espec√≠fico
'alert:warning'    // Alerta de aviso
'alerts:active'    // Lista de alertas ativos
'alerts:history'   // Hist√≥rico
'alerts:stats'     // Estat√≠sticas
'alerts:error'     // Erro no processamento
```

**Events (Client ‚Üí Server):**
```javascript
'alerts:request-active'    // Solicitar alertas ativos
'alerts:request-history'   // Solicitar hist√≥rico
'alerts:request-stats'     // Solicitar estat√≠sticas
```

### 2. **Client Libraries**

#### `clients/alert-client.js` - JavaScript/Web Client
```javascript
import AlertClient from './alert-client.js';

const client = new AlertClient('http://localhost:3000');

// Registrar callbacks
client.onAlertsUpdate((alerts, status) => {
  console.log('Alertas:', alerts);
});

client.onCriticalAlert((alert) => {
  console.log('CR√çTICO:', alert);
  // Mostrar toast/notifica√ß√£o
});

client.onStats((stats) => {
  console.log('Stats:', stats);
});
```

---

## üöÄ Setup & Configuration

### Passo 1: Configurar AlertManager para webhooks
```bash
cd /home/edenilson/eddie-auto-dev/estou-aqui/backend

# O script configura AlertManager para enviar para ambos:
# - Agent API (:8503)
# - Estou Aqui (:3000)
# OBS: usa `127.0.0.1` para evitar resolu√ß√µes de `localhost` em containers
bash scripts/setup-alert-integration.sh
```

#### Troubleshooting: 405 Method Not Allowed (AlertManager ‚Üí Estou-Aqui)
- Sintoma: AlertManager registra `405 Method Not Allowed` ao postar em `http://localhost:3000/api/alerts/webhook`.
- Causas comuns:
  - AlertManager est√° dentro de um container e `localhost` aponta para o pr√≥prio container (chave: use `127.0.0.1` ou o IP do host).
  - Outro servi√ßo escuta na porta 3000 (Nginx/Grafana) e responde com 405.
  - O backend estava parado no momento do envio; o proxy respondeu com 405/erro gen√©rico.
- Corre√ß√£o r√°pida:
  1. Atualize o webhook do AlertManager para `http://127.0.0.1:3000/api/alerts/webhook` **ou** para `http://127.0.0.1:3456/api/alerts/webhook` (use 3456 to target the backend process directly when nginx serves the SPA on :3000). (script updated to prefer :3456).
  2. Verifique se o `estou-aqui` backend est√° ativo: `curl -sS http://127.0.0.1:3000/health` (esperado 200).
  3. Confirme que nada mais est√° escutando na porta 3000: `sudo ss -ltnp | grep :3000`.
  4. Recarregue o AlertManager: `sudo systemctl reload alertmanager` and re-run the test alert.
- Recomenda√ß√£o: adicionar testes automatizados (inclu√≠dos abaixo) para evitar regress√µes.

#### Testes inclu√≠dos
- `tests/test_homelab_agent_registration.py` ‚Äî teste `pytest -m integration` que valida `advisor_api_registration_status == 1` no agent `/metrics` (usa `HOMELAB_HOST` para apontar o host remoto).  
- Como executar localmente (homelab access):
  - HOMELAB_HOST=192.168.15.2 pytest -q -m integration tests/test_homelab_agent_registration.py
  - No CI esses testes ficam marcados como `integration` (executar explicitamente quando necess√°rio).


### Passo 2: Iniciar o Backend do Estou Aqui
```bash
cd /home/edenilson/eddie-auto-dev/estou-aqui/backend

npm install
npm start
# ou
PORT=3000 npm start
```

O backend agora estar√°:
- ‚úÖ Recebendo webhooks em `/api/alerts/webhook`
- ‚úÖ Transmitindo via Socket.io no `/alerts`
- ‚úÖ Publicando no Agent Bus

### Painel Grafana ‚Äî saneamento de alertas
- Novo painel `Active Prometheus Alerts` foi adicionado ao dashboard `homelab-copilot-agent` para visualizar alertas `firing` relacionados ao homelab e ao homelab-advisor.
- Use o link `Open Alertmanager` no pr√≥prio painel para abrir o Alertmanager e silenciar/rever notifica√ß√µes rapidamente.

#### Provisionar Contact Point no Grafana (Estou‚ÄëAqui webhook)
- Objetivo: configurar o Grafana para enviar notifica√ß√µes/alertas diretamente ao backend `estou-aqui` via `POST /api/alerts/grafana-webhook`.

Op√ß√µes:
1) Runtime (via API Grafana):
```bash
curl -sS -u <grafana_user>:<grafana_pass> \
  -H "Content-Type: application/json" \
  -d '{"name":"Estou-Aqui Backend","type":"webhook","settings":{"url":"http://127.0.0.1:3456/api/alerts/grafana-webhook","httpMethod":"POST","uploadImage":false},"disableResolveMessage":false}' \
  -X POST http://<grafana-host>:3000/api/v1/provisioning/contact-points
```
2) Provisioning (arquivo) ‚Äî opcional: adicionar um arquivo de provisioning `contact-points` na pasta de provisioning do Grafana (ex.: `/home/homelab/monitoring/grafana/provisioning/`) conforme sua estrat√©gia de provisionamento.

Valida√ß√£o:
- Listar contact points provisionados:
  `curl -sS -u <grafana_user>:<grafana_pass> http://<grafana-host>:3000/api/v1/provisioning/contact-points | jq '.'`
- Confirmar que o Contact Point aparece com `url: http://127.0.0.1:3456/api/alerts/grafana-webhook`.

Teste E2E (r√°pido):
1. Criar/acionar uma regra de alerta no Grafana que notifique o Contact Point (ou usar o bot√£o "Test" no UI se dispon√≠vel).
2. OU simular o payload do Grafana diretamente no backend (exemplo abaixo):
```bash
curl -sS -X POST http://<estou-aqui-host>:3456/api/alerts/grafana-webhook \
  -H 'Content-Type: application/json' \
  -d '{"title":"High memory usage","ruleName":"HighMemoryUsage","state":"alerting","message":"Memory > 90%","evalMatches":[{"metric":"memory_total","value":93,"tags":{"instance":"homelab"},"time":"2026-02-16T22:00:00.000Z"}],"tags":{"severity":"critical"},"ruleUrl":"http://grafana/alert/1"}'
```
3. Verificar backend/homelab:
  - `curl http://<estou-aqui-host>:3456/api/alerts/active | jq` ‚Äî o alerta deve aparecer como `HighMemoryUsage`.
  - Conferir `journalctl -u estouaqui-backend` e `curl http://<homelab>:8085/metrics | grep advisor` conforme necess√°rio.

Observa√ß√µes operacionais:
- Use `127.0.0.1:3456` para apontar diretamente ao processo backend quando nginx serve SPA em `:3000`.
- Se Grafana estiver containerizada, prefira provisioning via API ou um arquivo de provisioning que seja aplicado no container host.

- Boas pr√°ticas ao saneamento:
  - Priorize alertas `critical` para investiga√ß√£o imediata; `warning` pode ser agrupado e avaliado durante manuten√ß√£o.
  - Ajuste `duration`/`thresholds` nas regras do Prometheus (em `/etc/prometheus/rules/`) em vez de apenas no dashboard ‚Äî isso reduz ru√≠do globalmente.
  - Sempre documente altera√ß√µes de regras em `ALERT_INTEGRATION_GUIDE.md` e revalide com um teste de integra√ß√£o (ex.: `tests/test_homelab_agent_registration.py`).

### Passo 3: Integrar no Frontend

#### Para Web (React/Vue/Vanilla JS):
```html
<script src="https://cdn.socket.io/4.5.4/socket.io.min.js"></script>
<script type="module">
  import AlertClient from './alert-client.js';
  
  const alerts = new AlertClient('http://localhost:3000');
  
  alerts.onCriticalAlert((alert) => {
    // Mostrar badge/notifica√ß√£o cr√≠tica
    updateUI(alert);
  });
</script>
```

#### Para Flutter:
```dart
import 'package:socket_io_client/socket_io_client.dart' as IO;

class AlertsPanel extends StatefulWidget {
  @override
  _AlertsPanelState createState() => _AlertsPanelState();
}

class _AlertsPanelState extends State<AlertsPanel> {
  late IO.Socket socket;
  List<Map> alerts = [];

  @override
  void initState() {
    super.initState();
    _initializeSocket();
  }

  void _initializeSocket() {
    socket = IO.io('http://localhost:3000/alerts',
      IO.OptionBuilder()
        .setTransports(['websocket'])
        .disableAutoConnect()
        .build()
    );

    socket.on('connect', (_) {
      print('‚úÖ Connected to alerts');
      socket.emit('alerts:request-active');
    });

    socket.on('alerts:update', (data) {
      setState(() {
        alerts = List<Map>.from(data['alerts'] ?? []);
      });
    });

    socket.on('alert:critical', (alert) {
      _showCriticalAlert(alert);
    });

    socket.connect();
  }

  void _showCriticalAlert(Map alert) {
    // Mostrar notifica√ß√£o cr√≠tica
    ScaffoldMessenger.of(context).showSnackBar(
      SnackBar(
        content: Text('üö® ${alert['summary']}'),
        backgroundColor: Colors.red,
        duration: Duration(seconds: 10),
      ),
    );
  }

  @override
  Widget build(BuildContext context) {
    return ListView(
      children: alerts.map((alert) => 
        AlertCard(alert: alert)
      ).toList(),
    );
  }

  @override
  void dispose() {
    socket.disconnect();
    super.dispose();
  }
}

class AlertCard extends StatelessWidget {
  final Map alert;

  const AlertCard({required this.alert});

  @override
  Widget build(BuildContext context) {
    final severity = alert['severity'] ?? 'unknown';
    final icon = severity == 'critical' ? 'üö®' : severity == 'warning' ? '‚ö†Ô∏è' : '‚ÑπÔ∏è';
    final color = severity == 'critical' ? Colors.red : Colors.orange;

    return Card(
      color: color.withOpacity(0.1),
      child: ListTile(
        leading: Text(icon, style: TextStyle(fontSize: 24)),
        title: Text(alert['summary'] ?? 'Alert'),
        subtitle: Text(alert['description'] ?? ''),
        trailing: Text(
          alert['status'] == 'firing' ? 'üî¥ Ativo' : 'üü¢ Resolvido',
          style: TextStyle(fontSize: 12),
        ),
      ),
    );
  }
}
```

---

## üìä Alert Rules

Atualmente, 4 regras monitorando (infra geral):

| Alerta | Threshold | Severity | Duration |
|--------|-----------|----------|----------|
| `DiskUsageHigh` | Disco < 20% livre | warning | 5 minutos |
| `DiskUsageCritical` | Disco < 10% livre | critical | 1 minuto |
| `HighCPUUsage` | CPU idle < 15% | warning | 5 minutos |
| `HighMemoryUsage` | Mem√≥ria > 85% | warning | 5 minutos |

**Arquivo:** `/etc/prometheus/rules/homelab-alerts.yml`

---

### homelab-advisor (agent-specific rules)

As regras do *Homelab Advisor* monitoram a disponibilidade e integridade do agente. Essas regras foram ajustadas para reduzir ru√≠do ‚Äî o `heartbeat` agora tolera at√© 5 minutos sem atualiza√ß√£o e as regras incluem `runbook_url` para triagem r√°pida.

| Alerta | Express√£o / Trigger | Severity | For |
|--------|---------------------|----------|-----|
| `HomelabAdvisorMissingHeartbeat` | time() - advisor_heartbeat_timestamp > 300 | critical | 2m |
| `HomelabAdvisorNotRegistered` | advisor_api_registration_status == 0 | warning | 5m |
| `HomelabAdvisorReportErrors` | increase(advisor_api_reports_total{status="error"}[5m]) > 0 | warning | 5m |

**Arquivo:** `/etc/prometheus/rules/homelab-advisor-alerts.yml`

Notas:
- `runbook_url` foi adicionado √†s anota√ß√µes das regras para direcionar operadores √†s instru√ß√µes de resolu√ß√£o.
- Recomenda√ß√£o: n√£o baixe o `heartbeat` para valores baixos (<2 min) em ambientes com poss√≠veis GC/pauses ‚Äî 5 minutos √© um compromisso razo√°vel para reduzir falsos positivos.
- Ap√≥s alterar regras, recarregue o Prometheus: `sudo systemctl reload prometheus` e verifique com `http://127.0.0.1:9090/api/v1/rules`.


---

## üß™ Testing

### Teste 1: Enviar alerta via curl
```bash
curl -X POST http://localhost:3000/api/alerts/webhook \
  -H "Content-Type: application/json" \
  -d '{
    "groupLabels": {
      "alertname": "TestAlert"
    },
    "status": "firing",
    "alerts": [
      {
        "status": "firing",
        "labels": {
          "alertname": "TestAlert",
          "severity": "warning",
          "instance": "test"
        },
        "annotations": {
          "summary": "Este √© um alerta de teste",
          "description": "Teste da integra√ß√£o de alertas"
        },
        "startsAt": "2026-02-16T14:30:00Z",
        "endsAt": "0001-01-01T00:00:00Z"
      }
    ]
  }'
```

### Teste 2: Verificar alertas ativos
```bash
curl http://localhost:3000/api/alerts/active | jq
```

### Teste 3: Verificar estat√≠sticas
```bash
curl http://localhost:3000/api/alerts/stats | jq
```

### Teste 4: Socket.io real-time (usando socket.io-client)
```bash
npm install -g socket.io-client-cli
socketio client --url http://localhost:3000/alerts \
  --events "alerts:update,alert:critical" \
  --emit "alerts:request-active"
```

---

## üìà Monitoring & Logs

### Ver logs do backend
```bash
cd estou-aqui/backend
npm start  # Ver√° eventos de alertas no console
```

### Ver logs do AlertManager
```bash
sudo journalctl -u alertmanager -f
```

### Verificar conex√µes Socket.io
```bash
# No navegador console ou terminal:
socket.on('connect', () => console.log('Connected!'));
socket.emit('alerts:request-active');
```

---

## ÔøΩÔ∏è Runbooks ‚Äî Homelab Advisor

### Heartbeat troubleshooting {#heartbeat-troubleshooting}
- Symptom: `HomelabAdvisorMissingHeartbeat` firing.
- Checar m√©tricas:
  - curl http://<homelab>:8085/metrics | grep advisor_heartbeat_timestamp
  - Verifique se o valor foi atualizado recentemente (timestamp unix).
- Logs e servi√ßos:
  - sudo journalctl -u homelab_copilot_agent -f
  - docker ps / docker logs homelab-copilot-agent
- Poss√≠veis a√ß√µes:
  1. Reinicie o agent: `sudo systemctl restart homelab_copilot_agent`.
  2. Se o agente estiver em container, confirme `API_BASE_URL` e conectividade ao host (`curl -sS http://172.17.0.1:8503/health`).
  3. Aumente toler√¢ncia do heartbeat em Prometheus se o ambiente sofrer pausas ocasionalmente.

### Registration failure {#registration-failure}
- Symptom: `HomelabAdvisorNotRegistered` firing.
- Checar m√©tricas:
  - curl http://<homelab>:8085/metrics | grep advisor_api_registration_status
- Verificar IPC/API:
  - curl -sS http://127.0.0.1:8503/health
  - Confirme que o container tem a vari√°vel `API_BASE_URL` apontando para o host gateway (ex: `http://172.17.0.1:8503`).
  - Se o backend estiver rodando em container, defina `AGENT_BUS_URL=http://172.17.0.1:8503` para garantir que os webhooks sejam publicados no mesmo Agent Bus que os agentes consultam.
- A√ß√µes r√°pidas:
  1. Ajuste systemd override para passar `API_BASE_URL` e reinicie o servi√ßo.
  2. Revise logs do agent para `Registrado na API principal via IPC`.

### IPC / Network troubleshooting {#ipc-network-troubleshooting}
- Symptom: `advisor_ipc_ready == 0` or `IPC init failed: could not translate host name` in agent logs.
- Causas comuns:
  - O container do agent n√£o est√° na mesma rede Docker que o Postgres (hostname `eddie-postgres` n√£o resolvido).
  - `DATABASE_URL` configurado com host/porta incorretos ou credenciais inv√°lidas.
- A√ß√µes imediatas:
  1. Use o sample systemd em `scripts/systemd/homelab_copilot_agent.service.sample` (inclui `--network homelab_monitoring` e `DATABASE_URL` apontando para `eddie-postgres`).
  2. Verifique conectividade DNS/TCP a partir do container: `docker exec <agent-cid> python3 -c "import socket; socket.create_connection(('eddie-postgres',5432),2)"`
  3. Confirme credenciais: `docker exec eddie-postgres env | grep POSTGRES_PASSWORD` e ajuste `DATABASE_URL` conforme necess√°rio.
  4. Reinicie o servi√ßo systemd que inicia o container (`sudo systemctl daemon-reload && sudo systemctl restart homelab_copilot_agent`).
- Comportamento esperado: ap√≥s corre√ß√£o, `curl -sS http://127.0.0.1:8085/health` deve retornar `"ipc_available": true` e `advisor_ipc_ready` deve ser exportada como m√©trica.
- Nota operacional: prefira injetar senha via Secrets Agent (n√£o hardcode).

### Reporting errors {#reporting-errors}
- Symptom: `HomelabAdvisorReportErrors` firing.
- Checar m√©tricas:
  - curl http://<homelab>:8085/metrics | grep advisor_api_reports_total
- Logs:
  - sudo journalctl -u homelab_copilot_agent -u -f | grep report
- A√ß√µes:
  1. Investigar payloads/Endpoints respons√°veis por `status=error`.
  2. Reprocessar falhas ou aplicar backoff/retry no agent se for transit√≥rio.

---

## ÔøΩüîó Integration with Agent Bus

Alertas s√£o automaticamente publicados no Agent Communication Bus:

```javascript
// Automaticamente feito pelo alerting.js
bus.publish(
  MessageType.ALERT,
  'estou-aqui-backend',
  'monitoring',
  '[CRITICAL] Disk usage critical',
  {
    alert_name: 'DiskUsageCritical',
    severity: 'critical',
    instance: 'homelab',
    group_labels: { ... }
  }
)
```

**Verificar mensagens no bus:**
```bash
curl http://localhost:8503/interceptor/conversations/active | jq
```

---

## üîÑ Workflow Completo

1. **Prometheus** coleta m√©tricas a cada 15s
2. **AlertManager** avalia regras a cada 60s
3. **Alerta dispara** ‚Üí AlertManager envia webhook
4. **Backend recebe** ‚Üí processa e cacheia
5. **Socket.io emite** ‚Üí todos os clientes recebem
6. **Agent Bus publica** ‚Üí agentes s√£o notificados
7. **UI atualiza** ‚Üí pain√©is mostram alerta em tempo real

---

## üö® Pr√≥ximas Melhorias

- [ ] Integra√ß√£o com Telegram/Email para cr√≠ticos
- [ ] Dashboard Grafana com alertas em tempo real
- [ ] Hist√≥rico persistente de alertas (DB)
- [ ] Regras customiz√°veis via API
- [ ] Grouping inteligente de alertas correlacionados
- [ ] Webhook com autentica√ß√£o (JWT)
- [ ] Retry autom√°tico de falhas

---

## üìû API Reference

Ver [ALERT_API.md](./ALERT_API.md) para documenta√ß√£o completa de endpoints.

---

**Status:** ‚úÖ **PRODUCTION READY**

Integra√ß√£o testada e validada. Pronta para produ√ß√£o.

*Last updated: 2026-02-16T14:30:00Z*
