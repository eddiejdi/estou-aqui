# LLM-Optimizer v2.3 ‚Äî OpenAI-compatible Proxy para Ollama

**Data:** 20 de fevereiro de 2026  
**Status:** ‚úÖ Produ√ß√£o  
**Porta:** 8512  
**Vers√£o:** 2.3.0  

## Vis√£o Geral

O LLM-Optimizer √© um proxy FastAPI que encaminha requisi√ß√µes OpenAI-compatible para o Ollama, aplicando estrat√©gias inteligentes de otimiza√ß√£o baseadas no tamanho do contexto. Especialmente desenvolvido para **suportar CLINE com tool-calling** via qwen3:4b.

```
CLINE ‚Üí http://192.168.15.2:8512/v1 ‚Üí LLM-Optimizer ‚Üí Ollama :11434
                    ‚Üì
    [Estrat√©gia de Otimiza√ß√£o]
    ‚îú‚îÄ A: < 2000 tokens ‚Üí qwen3:4b (direto)
    ‚îú‚îÄ B: 2000-6000 tokens ‚Üí qwen3:0.6b (modelo leve + mais r√°pido)
    ‚îî‚îÄ C: > 6000 tokens ‚Üí Map-Reduce paralelo (qwen3:0.6b MAP + qwen3:4b REDUCE)
                    ‚Üì
    [Smart Truncation para Tool-Calling]
    ‚îú‚îÄ Preserva tool definitions (CLINE XML tags)
    ‚îú‚îÄ Sanitiza content array ‚Üí string (multimodal)
    ‚îî‚îÄ Converte roles inv√°lidos ‚Üí suportados pelo Ollama
```

## Hist√≥rico de Vers√µes

### v2.3.0 (20 fev 2026 ‚Äî Corre√ß√£o cr√≠tica)
**Problema resolvido:**
- üêõ **Cannot read properties of undefined (reading 'type')** em mensagens multimodais

**Corre√ß√µes implementadas:**
- ‚úÖ Guards defensivos em `safe_get_content_text()` ‚Äî valida `item['type']` antes de acessar
- ‚úÖ Valida√ß√£o de schema OpenAI em `validate_openai_response()` ‚Äî garante campos obrigat√≥rios
- ‚úÖ Fallback robusto com `create_fallback_response()` ‚Äî resposta v√°lida mesmo em erro
- ‚úÖ Logging estruturado de erros de schema (`schema_errors` metric)
- ‚úÖ Tratamento de items nulos/inv√°lidos em content arrays

**Testes de contrato:**
- ‚úÖ Content string simples
- ‚úÖ Content array multimodal (gatilho do bug original)
- ‚úÖ Role tool/function (CLINE tool-calling)

**Deploy:**
```bash
./scripts/deploy_llm_optimizer.sh
```

**Arquivos:**
- `scripts/llm_optimizer_v2.3.py` ‚Äî proxy completo
- `scripts/test_llm_optimizer_contract.py` ‚Äî suite de testes
- `scripts/deploy_llm_optimizer.sh` ‚Äî deploy automatizado

### v2.2.0 (Produ√ß√£o ‚Äî 20 fev 2026)
**Melhorias:**
- ‚úÖ Timeouts aumentados: `600s ‚Üí 1200s` (20 min max)
- ‚úÖ `timeout_keep_alive` configurado no Uvicorn
- ‚úÖ Suporte para requisi√ß√µes complex do CLINE (MAP-Reduce com 3+ chunks)
- ‚úÖ Sincronizado com CLINE timeout: `1.200.000 ms`

**Testes:**
- ‚úÖ Req 1: fallback direto (5 min) ‚Üí resposta com tool-calling
- ‚úÖ Req 2: Map-Reduce 1 chunk (15 min) ‚Üí resposta com tool-calling
- ‚úÖ Req 3: Map-Reduce 3 chunks (6.5 min) ‚Üí resposta com tool-calling

**M√©tricas acumuladas:**
- 5 requisi√ß√µes bem-sucedidas
- 0 erros (400/500)
- 76.337 tokens salvos
- 5/5 tool-calling detectados
- 9 smart truncations executadas

### v2.1.0 (20 fev 2026 15:28)
**Melhorias:**
- ‚úÖ Sanitiza√ß√£o de mensagens CLINE
- ‚úÖ Content array ‚Üí string (multimodal support)
- ‚úÖ Roles inv√°lidos ‚Üí user/assistant
- ‚úÖ Error logging melhorado (body do erro 400)
- ‚úÖ Strategy A streaming sanitizado
- ‚úÖ Fix: 400 Bad Request resolvido

**Diagn√≥stico:**
- Problema: Ollama retornava 400 porque CLINE envia campos extras (tool_calls, roles=tool, content=array)
- Solu√ß√£o: sanitize_messages() limpa payload antes de enviar ao Ollama
- Resultado: 100% de sucesso nas 3 requisi√ß√µes teste

### v2.0.0 (20 fev 2026 14:56)
**Melhorias iniciais:**
- ‚úÖ Smart truncation com preserva√ß√£o de tool definitions
- ‚úÖ REDUCE usa qwen3:4b (foi 0.6b)
- ‚úÖ Tool-calling detection autom√°tico
- ‚úÖ Limites de truncamento aumentados (6k ‚Üí 8k+ chars)
- ‚úÖ Prometheus /metrics endpoint
- ‚úÖ Contexto num_ctx aumentado: 4096 ‚Üí 8192

## Configura√ß√£o

### Instala√ß√£o
```bash
# J√° instalado em /home/homelab/llm-optimizer/
# Depend√™ncias:
pip install fastapi uvicorn httpx pydantic prometheus-client
```

### Systemd Service
```ini
[Unit]
Description=LLM Optimizer Proxy for Ollama
After=network.target

[Service]
Type=notify
User=homelab
WorkingDirectory=/home/homelab/llm-optimizer
ExecStart=/home/homelab/llm-optimizer/venv/bin/python llm_optimizer.py
Restart=always
RestartSec=5
SyslogIdentifier=llm-optimizer

[Install]
WantedBy=multi-user.target
```

### Endpoints
| Endpoint | M√©todo | Descri√ß√£o |
|----------|--------|-----------|
| `/v1/chat/completions` | POST | ChatCompletion OpenAI-compatible |
| `/v1/models` | GET | Lista modelos dispon√≠veis |
| `/health` | GET | Health check + vers√£o + stats |
| `/metrics` | GET | Prometheus metrics (text/plain) |

### CLINE Configuration
**Arquivo:** `~/.cline/data/globalState.json`

```json
{
  "openAiBaseUrl": "http://192.168.15.2:8512/v1",
  "actModeOpenAiModelId": "qwen3:4b",
  "planModeOpenAiModelId": "qwen3:4b",
  "actModeApiProvider": "openai",
  "ollamaApiOptionsCtxNum": 8192,
  "requestTimeoutMs": 1200000
}
```

## Estrat√©gias de Otimiza√ß√£o

### Strategy A: Direto (< 2000 tokens)
- Modelo: `qwen3:4b`
- Contexto: 8192 tokens
- Timeout: 10 min

**Exemplo:** Requisi√ß√£o simples do CLINE.

### Strategy B: Modelo Leve (2-6K tokens)
- Modelo: `qwen3:0.6b` (troca do 4b)
- Contexto: 8192 tokens
- Timeout: 10 min
- **Benef√≠cio:** CPU mais r√°pido, tokens salvos

**Exemplo:** 5000 tokens ‚Üí usa 0.6b (mais r√°pido).

### Strategy C: Map-Reduce Paralelo (> 6K tokens)
1. **MAP** ‚Äî Sumariza chunks em paralelo com `qwen3:0.6b`
   - At√© 4 workers paralelos
   - ~30-60s por chunk (CPU-only)

2. **REDUCE** ‚Äî Sintetiza com `qwen3:4b` usando resumos
   - Contexto: 8192 tokens
   - Smart truncation preserva tool definitions
   - ~5-10 min

**Exemplo:** 17.5K tokens (4 mensagens) ‚Üí MAP 3 chunks + REDUCE.

## Smart Truncation (v2.1+)

Problema: CLINE envia ~54K char system prompt ‚Üí truncamento naive perde tool definitions ‚Üí modelo n√£o sabe gerar tool-calling v√°lido.

Solu√ß√£o: Truncamento inteligente que:
1. **Preserva in√≠cio** (identidade) ‚Äî 40% do budget
2. **Preserva tool definitions** ‚Äî 30% do budget
   - Detecta: `<tool_name>`, `execute_command`, `read_file`, etc.
   - Regex patterns para encontrar blocos `## Tools` ou `<tools>`
3. **Preserva fim** (instru√ß√µes de sa√≠da) ‚Äî 30% do budget

**Resultado:** 64.356 chars ‚Üí 8.444 chars com 100% das defini√ß√µes de tool intactas.

## Sanitiza√ß√£o de Mensagens (v2.1+)

CLINE envia mensagens em formato multimodal (n√£o suportado pelo Ollama):
```json
{
  "role": "tool",  // Inv√°lido para Ollama
  "content": [     // Array, n√£o string
    {"type":"text", "text":"..."},
    {"type":"image_url", "url":"..."}
  ],
  "tool_call_id": "..."  // Campo extra
}
```

Sanitiza√ß√£o aplica:
```python
def sanitize_messages(messages: list) -> list:
    # 1. Converte roles inv√°lidos: tool ‚Üí user, function ‚Üí user
    # 2. Converte content array ‚Üí string
    # 3. Remove campos extras: s√≥ mant√©m role + content
    # 4. Resultado: formato Ollama-compatible
```

**Benef√≠cio:** 0 erros 400 Bad Request ‚úÖ

## Prometheus Metrics

Scrape config (adicionar a `/etc/prometheus/prometheus.yml`):
```yaml
- job_name: "llm-optimizer"
  static_configs:
    - targets: ["localhost:8512"]
  scrape_interval: 15s
  scrape_timeout: 10s
  metrics_path: "/metrics"
```

### M√©tricas expostas
| M√©trica | Tipo | Descri√ß√£o |
|---------|------|-----------|
| `llm_optimizer_requests_total{strategy}` | counter | Requests por strategy (A/B/C/all) |
| `llm_optimizer_errors_total` | counter | Total de erros |
| `llm_optimizer_tokens_saved_total` | counter | Tokens salvos via otimiza√ß√£o |
| `llm_optimizer_dedup_hits_total` | counter | In-flight dedup cache hits |
| `llm_optimizer_tool_call_detected_total` | counter | Tool-calling requests detectados |
| `llm_optimizer_smart_truncations_total` | counter | Smart truncations executadas |
| `llm_optimizer_duration_seconds{strategy}` | gauge | Dura√ß√£o m√©dia por strategy |
| `llm_optimizer_up` | gauge | Service status (1=up, 0=down) |

### Exemplo de query Prometheus
```promql
# Total de requisi√ß√µes por strategy nos √∫ltimos 5min
rate(llm_optimizer_requests_total[5m]) by (strategy)

# Tokens economizados
llm_optimizer_tokens_saved_total

# Taxa de erro (deve ser 0)
rate(llm_optimizer_errors_total[5m])

# Dura√ß√£o m√©dia de requisi√ß√µes
llm_optimizer_duration_seconds
```

## Grafana Dashboard

**UID:** `homelab-session-monitor`  
**Vers√£o:** 2 (atualizado com pain√©is v2)  
**Se√ß√µes:**
1. ü§ñ Status do LLM-Optimizer (4 stat panels)
2. üìä Requests Timeline (Strategy A/B/C)
3. ‚è±Ô∏è Dura√ß√£o por Strategy
4. üíæ Mem√≥ria (cAdvisor)
5. üìù Logs (Loki)

**Acesso:** http://192.168.15.2:3002 (admin:Eddie@2026)

## Troubleshooting

### Request timed out
**Cen√°rio:** CLINE retorna "Request timed out" ap√≥s 5-10 min.

**Causa:** Requisi√ß√£o grande (Map-Reduce) demorou mais que timeout CLINE + LLM-Optimizer.

**Solu√ß√£o:**
```bash
# CLINE timeout
sed -i 's/"requestTimeoutMs": [0-9]*/"requestTimeoutMs": 1200000/' ~/.cline/data/globalState.json

# LLM-Optimizer timeout (v2.2)
grep "TIMEOUT_EACH" /home/homelab/llm-optimizer/llm_optimizer.py
# Deve ser 1200 (segundos)

# Reiniciar
sudo systemctl restart llm-optimizer
```

### 400 Bad Request

**Cen√°rio:** `[openai] 400 status code (no body)`

**Causa:** Mensagens CLINE com campos inv√°lidos.

**Solu√ß√£o:** Usar v2.1+ que sanitiza automaticamente.

**Verificar:**
```bash
journalctl -u llm-optimizer -f | grep -E "400|sanitize|ERROR"
curl http://192.168.15.2:8512/health | jq '.version'
# Deve ser "2.1.0" ou superior
```

### Ollama crash

**Cen√°rio:** Processo Ollama desaparece ap√≥s requisi√ß√£o.

**Solu√ß√£o:**
```bash
# Reiniciar Ollama
sudo systemctl restart ollama

# Carregar modelo novamente
ssh homelab@192.168.15.2 'ollama pull qwen3:4b'
```

## Performance Esperada

| Estrat√©gia | Tokens | Tempo | Modelo |
|-----------|--------|-------|--------|
| A | < 2K | 2-3 min | qwen3:4b |
| B | 2-6K | 2-4 min | qwen3:0.6b |
| C | > 6K | 10-15 min | MAP(0.6b) + REDUCE(4b) |

**Nota:** CPU-only no homelab. GPU aceleraria 3-5x.

## Desenvolvimento

### Build local
```bash
cd /home/homelab/llm-optimizer
python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn httpx pydantic prometheus-client
python llm_optimizer.py
# Acesso: http://localhost:8512/health
```

### Backup de vers√µes
```bash
ls -lh /home/homelab/llm-optimizer/llm_optimizer.py*
# llm_optimizer.py           (current)
# llm_optimizer.py.bak.v2    (v2.0 backup)
# llm_optimizer.py.bak.v2.1  (v2.1 backup)
```

### Rollback
```bash
cp /home/homelab/llm-optimizer/llm_optimizer.py.bak.v2.1 \
   /home/homelab/llm-optimizer/llm_optimizer.py
sudo systemctl restart llm-optimizer
```

## Pr√≥ximas Melhorias

- [ ] GPU acceleration (CUDA/ROCm) para 3-5x speedup
- [ ] Cache de resposts (Redis/memcached)
- [ ] Circuit breaker para Ollama offline
- [ ] Dynamic worker scaling baseado em CPU
- [ ] Suport para streaming de resposta (SSE)
- [ ] Rate limiting por cliente
- [ ] Authentication (API key)

## Refer√™ncias

- **LLM-Optimizer Code:** `/home/homelab/llm-optimizer/llm_optimizer.py`
- **Systemd Service:** `/etc/systemd/system/llm-optimizer.service`
- **Prometheus:** http://192.168.15.2:9090/targets?search=llm-optimizer
- **Grafana Dashboard:** http://192.168.15.2:3002/d/homelab-session-monitor
